* All sensors attributes are available with explanations on Carla docs website:
https://carla.readthedocs.io/en/latest/ref_sensors/
----------------------------------------------------------------------------------------------------
// Frames of reference
All the sensors use the UE coordinate system (x-forward, y-right, z-up), and return coordinates in local space. When using any visualization software, pay attention to its coordinate system. Many invert the Y-axis, so visualizing the sensor data directly may result in mirrored outputs. 
# New we must change from UE4's coordinate system to an "standard"
# camera coordinate system (the same used by OpenCV):

# ^ z                       . z
# |                        /
# |              to:      +-------> x
# | . x                   |
# |/                      |
# +-------> y             v y

# This can be achieved by multiplying by the following matrix:
# [[ 0,  1,  0 ],
#  [ 0,  0, -1 ],
#  [ 1,  0,  0 ]]

# Or, in this case, is the same as swapping:
# (x, y ,z) -> (y, -z, x)
point_in_camera_coords = np.array([
sensor_points[1],
sensor_points[2] * -1,
sensor_points[0]])

# Finally we can use our K matrix to do the actual 3D -> 2D.
points_2d = np.dot(K, point_in_camera_coords)

# Remember to normalize the x, y values by the 3rd value.
points_2d = np.array([
points_2d[0, :] / points_2d[2, :],
points_2d[1, :] / points_2d[2, :],
points_2d[2, :]])

# At this point, points_2d[0, :] contains all the x and points_2d[1, :]
# contains all the y values of our points. In order to properly
# visualize everything on a screen, the points that are out of the screen
# must be discarted, the same with points behind the camera projection plane.
points_2d = points_2d.T
intensity = intensity.T
points_in_canvas_mask = \
(points_2d[:, 0] > 0.0) & (points_2d[:, 0] < image_w) & \
(points_2d[:, 1] > 0.0) & (points_2d[:, 1] < image_h) & \
(points_2d[:, 2] > 0.0)
points_2d = points_2d[points_in_canvas_mask]
intensity = intensity[points_in_canvas_mask]

# Extract the screen coords (uv) as integers.
u_coord = points_2d[:, 0].astype(np.int)
v_coord = points_2d[:, 1].astype(np.int)

----------------------------------------------------------------------------------------------------
// RGB and Depth Cameras Calibration

# Build the K projection matrix:
# K = [[Fx,  0, image_w/2],
#      [ 0, Fy, image_h/2],
#      [ 0,  0,         1]]
image_w = camera_bp.get_attribute("image_size_x").as_int()
image_h = camera_bp.get_attribute("image_size_y").as_int()
fov = camera_bp.get_attribute("fov").as_float()
focal = image_w / (2.0 * np.tan(fov * np.pi / 360.0))

# In this case Fx and Fy are the same since the pixel aspect
# ratio is 1
K = np.identity(3)
K[0, 0] = K[1, 1] = focal
K[0, 2] = image_w / 2.0
K[1, 2] = image_h / 2.0
     
Where Cu and Cv represents the center point of the image.

Given a 2D point p2d = [u,v,1], your world point position P = [X,Y,Z] will be:

P = ( inv(K) * p2d ) * depth

where

normalized = (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1)
depth = 1000 * normalized     (in meters)
----------------------------------------------------------------------------------------------------
// Semantic Segmentation Camera 
The following tags are currently available on Carla Documentation website:
https://carla.readthedocs.io/en/latest/ref_sensors/#semantic-segmentation-camera

----------------------------------------------------------------------------------------------------
// DAVIS sensor
The DVS camera outputs a stream of events. An event e=(x,y,t,pol) is triggered at a pixel x, y at a timestamp t when the change in logarithmic intensity L reaches a predefined constant threshold C (typically between 15% and 30%).

L(x,y,t) - L(x,y,t- dt) = pol C
----------------------------------------------------------------------------------------------------
